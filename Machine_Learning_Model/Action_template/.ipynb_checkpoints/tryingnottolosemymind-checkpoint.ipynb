{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d7cf25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.4.1\n",
      "  Downloading tensorflow-2.4.1-cp37-cp37m-win_amd64.whl (370.7 MB)\n",
      "     -------------------------------------- 370.7/370.7 MB 4.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (1.15.0)\n",
      "Collecting h5py~=2.10.0\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 13.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (1.19.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (3.3.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (0.37.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (3.7.4.3)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (1.12.1)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (0.2.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (2.7.0)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "     ------------------------------------- 462.0/462.0 KB 14.1 MB/s eta 0:00:00\n",
      "Collecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp37-cp37m-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 12.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (0.15.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (3.19.1)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (1.1.2)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (1.1.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==2.4.1) (1.12)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.3.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (58.3.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.26.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.3.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (4.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kaitlin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.6.0)\n",
      "Installing collected packages: tensorflow-estimator, h5py, grpcio, gast, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.5.0\n",
      "    Uninstalling tensorflow-estimator-2.5.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.34.1\n",
      "    Uninstalling grpcio-1.34.1:\n",
      "      Successfully uninstalled grpcio-1.34.1\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.4.0\n",
      "    Uninstalling gast-0.4.0:\n",
      "      Successfully uninstalled gast-0.4.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.5.0\n",
      "    Uninstalling tensorflow-2.5.0:\n",
      "      Successfully uninstalled tensorflow-2.5.0\n",
      "Successfully installed gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c7e7912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84432908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('Iris_Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['yes', 'no' ])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "start_folder = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "372aff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    for sequence in range(no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c372a62",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_30308/666499558.py, line 73)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Kaitlin\\AppData\\Local\\Temp/ipykernel_30308/666499558.py\"\u001b[1;36m, line \u001b[1;32m73\u001b[0m\n\u001b[1;33m    count++\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "from custom.iris_lm_depth import from_landmarks_to_depth\n",
    "from videosource import FileSource, WebcamSource\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "left_eye_landmarks_id = np.array([33, 133])\n",
    "right_eye_landmarks_id = np.array([362, 263])\n",
    "\n",
    "dist_coeff = np.zeros((4, 1))\n",
    "\n",
    "YELLOW = (0, 255, 255)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (255, 0, 0)\n",
    "RED = (0, 0, 255)\n",
    "SMALL_CIRCLE_SIZE = 1\n",
    "LARGE_CIRCLE_SIZE = 2\n",
    "\n",
    "\n",
    "LEFT_EYE_LANDMARKS_ID = np.array([33, 133])\n",
    "RIGHT_EYE_LANDMARKS_ID = np.array([362, 263])\n",
    "\n",
    "POINTS_IDX = [33, 133, 362, 263]\n",
    "POINTS_IDX = list(set(POINTS_IDX))\n",
    "POINTS_IDX.sort()\n",
    "\n",
    "\n",
    "def __add_landmark_to_df(landmark, landmark_idx, df_headers, df_values):\n",
    "    \"\"\"Helper function that adds a landmark to the dataframe\"\"\"\n",
    "\n",
    "    df_headers.append(\"x{}\".format(landmark_idx))\n",
    "    df_headers.append(\"y{}\".format(landmark_idx))\n",
    "    df_headers.append(\"z{}\".format(landmark_idx))\n",
    "\n",
    "    df_values.append(landmark[0])\n",
    "    df_values.append(landmark[1])\n",
    "    df_values.append(landmark[2])\n",
    "\n",
    "\n",
    "def main1():\n",
    "    FIRST_TIME = True\n",
    "    frame_height, frame_width = (720, 1280)\n",
    "    source = WebcamSource(width=frame_width, height=frame_height)\n",
    "    image_size = (frame_width, frame_height)\n",
    "    count = 0\n",
    "\n",
    "    # pseudo camera internals\n",
    "    focal_length = frame_width\n",
    "\n",
    "    landmarks = None\n",
    "    smooth_left_depth = -1\n",
    "    smooth_right_depth = -1\n",
    "    smooth_factor = 0.1\n",
    "\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=False,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5,\n",
    "    ) as face_mesh:\n",
    "        for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "            for sequence in range(no_sequences):\n",
    "                # Loop through video length aka sequence length\n",
    "                for idx, (frame, frame_rgb) in enumerate(source):\n",
    "                    #only go 30 times\n",
    "                    count = count +1\n",
    "                    if count == 30:\n",
    "                        count = 0\n",
    "                        break\n",
    "#                     for idx, (frame, frame_rgb) in enumerate(source):\n",
    "                    results = face_mesh.process(frame_rgb)\n",
    "                    multi_face_landmarks = results.multi_face_landmarks\n",
    "\n",
    "                    if multi_face_landmarks:\n",
    "                        face_landmarks = results.multi_face_landmarks[0]\n",
    "                        landmarks = np.array(\n",
    "                            [(lm.x, lm.y, lm.z) for lm in face_landmarks.landmark]\n",
    "                        )\n",
    "                        landmarks = landmarks.T\n",
    "\n",
    "                        (\n",
    "                            left_depth,\n",
    "                            left_iris_size,\n",
    "                            left_iris_landmarks,\n",
    "                            left_eye_contours,\n",
    "                        ) = from_landmarks_to_depth(\n",
    "                            frame_rgb,\n",
    "                            landmarks[:, left_eye_landmarks_id],\n",
    "                            image_size,\n",
    "                            is_right_eye=False,\n",
    "                            focal_length=focal_length,\n",
    "                        )\n",
    "\n",
    "                        (\n",
    "                            right_depth,\n",
    "                            right_iris_size,\n",
    "                            right_iris_landmarks,\n",
    "                            right_eye_contours,\n",
    "                        ) = from_landmarks_to_depth(\n",
    "                            frame_rgb,\n",
    "                            landmarks[:, right_eye_landmarks_id],\n",
    "                            image_size,\n",
    "                            is_right_eye=True,\n",
    "                            focal_length=focal_length,\n",
    "                        )\n",
    "\n",
    "                        if smooth_right_depth < 0:\n",
    "                            smooth_right_depth = right_depth\n",
    "                        else:\n",
    "                            smooth_right_depth = (\n",
    "                                smooth_right_depth * (1 - smooth_factor)\n",
    "                                + right_depth * smooth_factor\n",
    "                            )\n",
    "\n",
    "                        if smooth_left_depth < 0:\n",
    "                            smooth_left_depth = left_depth\n",
    "                        else:\n",
    "                            smooth_left_depth = (\n",
    "                                smooth_left_depth * (1 - smooth_factor)\n",
    "                                + left_depth * smooth_factor\n",
    "                            )\n",
    "\n",
    "                        print(\n",
    "                            f\"depth in cm: {smooth_left_depth / 10:.2f}, {smooth_right_depth / 10:.2f}\"\n",
    "                        )\n",
    "                        print(f\"size: {left_iris_size:.2f}, {right_iris_size:.2f}\")\n",
    "\n",
    "                    if landmarks is not None:\n",
    "\n",
    "                        landmark_idx = 0\n",
    "                        df_headers = []\n",
    "                        df_values = []\n",
    "\n",
    "                        # add eye contours to dataframe\n",
    "                        eye_landmarks = np.concatenate(\n",
    "                            [\n",
    "                                right_eye_contours[0:17],\n",
    "                                left_eye_contours[0:17],\n",
    "                            ]\n",
    "                        )\n",
    "\n",
    "                        # add iris landmarks to dataframe\n",
    "                        iris_landmarks = np.concatenate(\n",
    "                            [\n",
    "                                right_iris_landmarks,\n",
    "                                left_iris_landmarks,\n",
    "                            ]\n",
    "                        )\n",
    "                        for landmark in iris_landmarks:\n",
    "\n",
    "                            __add_landmark_to_df(\n",
    "                                landmark, landmark_idx, df_headers, df_values)\n",
    "\n",
    "                            landmark_idx += 1\n",
    "\n",
    "                        for landmark in eye_landmarks:\n",
    "\n",
    "                            __add_landmark_to_df(\n",
    "                                landmark, landmark_idx, df_headers, df_values)\n",
    "\n",
    "                            landmark_idx += 1\n",
    "\n",
    "                        # add subset of facemesh to dataframe\n",
    "                        for ii in POINTS_IDX:\n",
    "\n",
    "                            landmark = (landmarks[0, ii],\n",
    "                                        landmarks[1, ii], landmarks[2, ii])\n",
    "                            __add_landmark_to_df(\n",
    "                                landmark, landmark_idx, df_headers, df_values)\n",
    "\n",
    "                            landmark_idx += 1\n",
    "\n",
    "                         # Export to CSV\n",
    "#                         with open(outputfilepath, mode='a', newline='') as f:\n",
    "#                             csv_writer = csv.writer(\n",
    "#                                 f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "#                             if (FIRST_TIME):\n",
    "#                                 csv_writer.writerow(df_headers)\n",
    "#                                 FIRST_TIME = False\n",
    "#                             csv_writer.writerow(df_values)\n",
    "\n",
    "                        # draw subset of facemesh\n",
    "                        for ii in POINTS_IDX:\n",
    "                            pos = (np.array(image_size) *\n",
    "                                   landmarks[:2, ii]).astype(np.int32)\n",
    "                            frame = cv2.circle(frame, tuple(\n",
    "                                pos), LARGE_CIRCLE_SIZE, GREEN, -1)\n",
    "\n",
    "                        # draw eye contours\n",
    "                        eye_landmarks = np.concatenate(\n",
    "                            [\n",
    "                                right_eye_contours[0:17],\n",
    "                                left_eye_contours[0:17],\n",
    "                            ]\n",
    "                        )\n",
    "                        for landmark in eye_landmarks:\n",
    "                            pos = (np.array(image_size) *\n",
    "                                   landmark[:2]).astype(np.int32)\n",
    "                            frame = cv2.circle(frame, tuple(\n",
    "                                pos), SMALL_CIRCLE_SIZE, RED, -1)\n",
    "\n",
    "                        # draw iris landmarks\n",
    "                        iris_landmarks = np.concatenate(\n",
    "                            [\n",
    "                                right_iris_landmarks[0:3],\n",
    "                                left_iris_landmarks[0:5],\n",
    "                            ]\n",
    "                        )\n",
    "                        for landmark in iris_landmarks:\n",
    "                            pos = (np.array(image_size) *\n",
    "                                   landmark[:2]).astype(np.int32)\n",
    "                            frame = cv2.circle(frame, tuple(\n",
    "                                pos), SMALL_CIRCLE_SIZE, YELLOW, -1)\n",
    "\n",
    "                        # write depth values into frame\n",
    "                        depth_string = \"{:.2f}cm, {:.2f}cm\".format(\n",
    "                            smooth_left_depth / 10, smooth_right_depth / 10\n",
    "                        )\n",
    "                        frame = cv2.putText(\n",
    "                            frame,\n",
    "                            depth_string,\n",
    "                            (50, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            1,\n",
    "                            GREEN,\n",
    "                            2,\n",
    "                            cv2.LINE_AA,\n",
    "                        )\n",
    "                        \n",
    "                     # NEW Apply wait logic\n",
    "                if count == 0: \n",
    "                    cv2.putText(frame, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(frame, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    source.show(frame)\n",
    "                    cv2.waitKey(2000)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    source.show(frame)\n",
    "                    \n",
    "                keypoints = df_values.to_numpy()\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(count))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "\n",
    "\n",
    "main1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68344361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
